import os
import cv2
import mediapipe as mp
import numpy as np
from PrepareData import X_train_combined, y_train_combined, X_test_combined, y_test_combined
from models.PCA_KNN import PCA_KNN

# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Mediapipe Face Detection
mp_face_detection = mp.solutions.face_detection
mp_drawing = mp.solutions.drawing_utils

# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏´‡πâ‡∏ù‡∏∂‡∏Å‡πÉ‡∏´‡∏°‡πà
def load_or_train_pca_knn():
    model_path = "pca_knn_model.pkl"
    if os.path.exists(model_path):
        print("üì• Loading existing PCA_KNN model...")
        return PCA_KNN.load_model(model_path)
    else:
        print("üöÄ Training new PCA_KNN model...")
        model = PCA_KNN()
        y_train = np.argmax(y_train_combined, axis=1) if len(y_train_combined.shape) > 1 else y_train_combined
        model.train(X_train_combined, y_train)
        model.save_model(model_path)
        return model

# ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•
pca_knn_model = load_or_train_pca_knn()

# ‡πÄ‡∏õ‡∏¥‡∏î‡∏Å‡∏•‡πâ‡∏≠‡∏á
cap = cv2.VideoCapture(0)

# ‡∏Å‡∏≥‡∏´‡∏ô‡∏î label ‡∏Ç‡∏≠‡∏á‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå
EMOTION_LABELS = ["Angry", "Sad", "Happy", "Neutral"]

with mp_face_detection.FaceDetection(min_detection_confidence=0.2) as face_detection:
    while True:
        ret, frame = cap.read()
        if not ret:
            break

        # ‡πÅ‡∏õ‡∏•‡∏á‡∏†‡∏≤‡∏û‡πÄ‡∏õ‡πá‡∏ô RGB
        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

        # ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤
        results = face_detection.process(rgb_frame)

        # ‡∏ñ‡πâ‡∏≤‡∏û‡∏ö‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤
        if results.detections:
            for detection in results.detections:
                bboxC = detection.location_data.relative_bounding_box
                h, w, _ = frame.shape
                x, y, w_box, h_box = int(bboxC.xmin * w), int(bboxC.ymin * h), int(bboxC.width * w), int(bboxC.height * h)

                # ‡∏Ñ‡∏£‡∏≠‡∏ö‡∏ï‡∏±‡∏î‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤
                face = frame[y:y+h_box, x:x+w_box]

                if face.shape[0] > 0 and face.shape[1] > 0:
                    # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô grayscale ‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏±‡∏ö‡∏Ç‡∏ô‡∏≤‡∏î
                    face_gray = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)
                    face_resized = cv2.resize(face_gray, (48, 48))

                    # ‡πÅ‡∏™‡∏î‡∏á‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å resize
                    cv2.imshow("Resized Face", face_resized)

                    # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô numpy array ‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏±‡∏ö‡∏£‡∏π‡∏õ‡∏£‡πà‡∏≤‡∏á
                    face_input = face_resized.flatten().reshape(1, -1)

                    # ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå
                    predicted_label = pca_knn_model.predict(face_input)[0]
                    emotion_text = EMOTION_LABELS[predicted_label]

                    # ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢
                    print(f"Predicted Label: {predicted_label} -> {emotion_text}")

                    # ‡∏ß‡∏≤‡∏î‡∏Å‡∏£‡∏≠‡∏ö‡∏£‡∏≠‡∏ö‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤
                    cv2.rectangle(frame, (x, y), (x + w_box, y + h_box), (0, 255, 0), 2)

                    # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏´‡πâ‡∏´‡πà‡∏≤‡∏á‡∏à‡∏≤‡∏Å‡∏Å‡∏£‡∏≠‡∏ö
                    cv2.putText(frame, emotion_text, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)
        # ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•
        cv2.imshow("Face & Emotion Detection", frame)

        # ‡∏Å‡∏î 'q' ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏¥‡∏î
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break

cap.release()
cv2.destroyAllWindows()

